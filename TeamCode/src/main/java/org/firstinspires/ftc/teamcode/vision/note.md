# FTC视觉抓取系统 - 工程笔记

**队伍:** [27570]
**日期:** 2025-06-09
**系统版本:** 1.0 (基于 BlueDarkUP 2025/6 版本)

## 1. 系统概述

本工程笔记详细介绍了我们团队视觉抓取系统的设计、功能和创新点。该系统的主要目标是为我们的FTC机器人提供强大且智能的目标检测和定位能力，从而在自动和手动遥控（TeleOp）阶段都能高效、准确地抓取比赛元素。

该系统通过EasyOpenCV利用OpenCV库处理来自网络摄像头的图像。它能够识别指定的目标（例如，特定颜色的立方体），计算其相对于机器人的真实世界位置和姿态，并提供可操作的数据以指导抓取机构和驾驶员决策。

## 2. 核心架构亮点

该系统构建于模块化和分层架构之上，增强了可维护性、可测试性，并实现了清晰的关注点分离：

*   **配置层 (`VisionConstants.java`):** 集中管理所有可调参数，从摄像头设置、HSV颜色范围到物理校准值和物体尺寸阈值。这使得快速调整和优化成为可能，而无需修改核心逻辑。
*   **图像处理管线 (`SamplePipeline.java`):** 视觉系统的核心，负责逐帧分析。包括图像缩放、颜色分割 (HSV)、降噪（中值滤波、形态学操作）、轮廓检测以及基于面积、长宽比和其他几何属性的复杂筛选。
*   **计算核心 (`VisionGraspingCalculator.java`):** 将原始视觉数据 (`VisionTargetResult`) 转换为精确的舵机指令 (`GraspingTarget`)。该层集成了先进的数学修正，以解决现实世界中的光学和机械非线性问题。
*   **API门面 (`VisionGraspingAPI.java`):** 为复杂的底层视觉操作提供了一个简化和清晰的接口。它管理摄像头初始化、管线设置以及视觉处理线程与机器人主控制循环之间的线程安全数据交换。
*   **数据中心设计 (`Data` 目录中的类):** 利用不可变数据对象（`VisionTargetResult`, `GraspingTarget`, `DetectedCube` 等）确保线程安全以及模块间清晰、一致的数据传输。
*   **应用层 (`VisionGraspingTeleOp.java`):** 展示了在TeleOp模式下视觉系统的最佳实践实现，演示了如何集成视觉反馈以增强驾驶员控制和自动辅助功能。

## 3. 系统亮点与技术创新

我们的视觉抓取系统在设计和实现上融合了多项技术创新，旨在应对FTC竞赛中复杂多变的挑战，提升机器人在赛场上的表现。

### 3.1. 智能驾驶辅助与直观反馈机制

为了有效减轻驾驶员在紧张比赛中的认知负担，并提升手动操作的效率与精准度，系统内置了智能辅助功能。它不仅仅简单地告知目标是否存在，更能提供具有实际指导意义的反馈信息：
*   **动态目标建议 (`nextTargetHorizontalOffsetCm`):** 当首选目标并非最佳，或视野内出现多个可选目标时，系统能够智能计算出机器人需要横向平移的精确距离（厘米）。这一功能极大地简化了驾驶员的瞄准过程，帮助他们快速、准确地将机器人对准下一个最合适的目标或备选目标。
*   **可抓取目标概览 (`graspableTargetsInZone`):** 系统会实时统计并告知驾驶员在预设的“可抓取区域”内有多少个符合条件的有效目标。这使得驾驶员能够基于场上局势和目标分布，迅速做出更明智的战术决策。
通过这些功能，视觉系统从一个传统意义上的被动传感器，转变为一个能够与驾驶员协同作战的“智能副驾”，显著提升了人机交互的效率和比赛成绩。

### 3.2. 高鲁棒性的多级目标识别与筛选策略

在复杂且动态变化的比赛场地中，准确识别出目标物体是一项核心挑战。为此，`SamplePipeline.java` 实现了一套精密的多级筛选与识别流程，确保了在各种视觉噪声、光照变化及背景干扰下的高识别率和可靠性：
1.  **精准颜色分割:** 基于预设的HSV颜色范围，精确分离出目标颜色区域。
2.  **图像净化处理:** 运用中值滤波和形态学操作（如腐蚀和膨胀）有效去除二值化图像中的噪声点，平滑目标边缘。
3.  **严格几何约束筛选:** 对检测到的轮廓进行多维度几何特性分析，包括面积（设定最小和最大阈值以排除过小或过大的物体）和长宽比（结合容差范围确保目标基本形状符合预期）。特别地，系统采用 `RotatedRect` （旋转矩形）来精确描述和测量可能倾斜的目标的实际尺寸和角度。
4.  **动态“可抓取区域”验证:** 系统会判断检测到的物体中心点是否落在根据机器人实际抓取能力动态计算出的“可抓取区域”内，确保只考虑机器人能够实际操作的目标。
5.  **智能候选目标评分机制:** 当视野内存在多个满足条件的目标时，系统会根据一套优先级标准（如距离作为首要标准，目标的对齐角度或自身姿态角度作为次要标准）对它们进行综合评分和排序，从而确保机器人总是优先处理最理想、最易于抓取的目标。

### 3.3. 精密的物理模型与误差补偿机制

为了克服简单几何投影带来的固有限制，并提升距离估算和最终抓取定位的绝对精度，`VisionGraspingCalculator.java` 中集成了一系列基于物理模型和经验数据的先进修正算法：
*   **非线性距离修正 (`DISTANCE_CORRECTION_EXPONENT`):** 考虑到物体在图像中的表观尺寸/位置与其真实三维空间距离之间并非简单的线性关系，此参数用于拟合和校正这种非线性效应。
*   **横向透视畸变补偿 (`LATERAL_DISTANCE_ATTENUATION_FACTOR`):** 修正由于透视效应导致的问题——即当物体偏离摄像头主光轴较远时，其在图像上的投影可能会使其看起来比实际距离更远或更近。此因子对此类横向位置相关的距离估算误差进行补偿。
*   **针对性瞄准校准 (`LEFT_SIDE_AIM_CORRECTION_DEGREES`):** 针对机器人结构或摄像头安装可能存在的系统性偏差（例如，当目标位于机器人左侧时，可能需要微调瞄准角度），此参数提供了一个特定的角度补偿值，以消除这种固定的瞄准误差。
这些精细的校正机制共同作用，显著提升了机器人对目标三维位置感知的准确性，从而大幅度提高了抓取任务的成功率。

### 3.3. 先进的视觉修正算法 (创新部分)

为了进一步提升目标定位的精度，特别是在复杂场景和不同距离下，系统可以引入更高级的视觉修正算法。`VisionGraspingCalculator.java` 中已有的 `DISTANCE_CORRECTION_EXPONENT` 为非线性距离修正提供了基础，以下将展开讨论此类修正及横向透视畸变补偿的原理与公式。

#### 3.3.1. 非线性距离修正

**问题描述:** 简单的线性模型（例如基于目标像素高度反比于距离）在整个视野深度范围内可能不够准确。透视效应、镜头畸变以及物体本身的复杂三维形状都可能导致目标在图像中的表观尺寸/位置与其真实三维空间距离之间呈现非线性关系。

**修正思路:** 采用非线性函数来拟合和校正这种关系。

**通用公式示例:**

1.  **基于幂函数的修正 (与 `DISTANCE_CORRECTION_EXPONENT` 相关):**
    `D_corrected = k * (D_raw ^ E) + C`
    其中:
    *   `D_corrected`: 修正后的距离。
    *   `D_raw`: 通过初步几何计算（如基于像素大小）得到的原始距离估算值。
    *   `E`: 距离修正指数 (即 `DISTANCE_CORRECTION_EXPONENT`)，通过实验标定得到，用于描述非线性程度。
    *   `k`: 缩放因子，通过实验标定。
    *   `C`: 偏移常数，通过实验标定。

2.  **基于多项式的修正 (例如二次多项式):**
    `D_corrected = a * D_raw^2 + b * D_raw + c`
    其中:
    *   `a`, `b`, `c`: 多项式系数，通过对一系列已知距离及其对应原始估算距离进行最小二乘拟合得到。

**实现要点:**
*   **标定:** 这些参数 (`k`, `E`, `C` 或 `a`, `b`, `c`) 必须通过精确的标定过程获得。这通常涉及在多个已知距离处放置目标，记录其在图像中的特征（如像素高度、面积或y坐标），然后拟合这些数据以确定最佳参数值。
*   **视野范围:** 修正模型应在机器人的有效工作视野范围内进行标定和验证。

#### 3.3.2. 横向透视畸变补偿

**问题描述:** 当目标物体偏离摄像头光轴中心较远时，透视效应会导致其横向尺寸或位置发生畸变。例如，同样宽度的物体在视野边缘看起来可能比在中心时更窄或更宽（取决于镜头类型和畸变特性），或者其感知的横向位置与实际的横向偏移不成线性比例。

**修正思路:** 建立一个模型来描述图像中观察到的横向位置（或宽度）与物体实际横向位置（或宽度）之间的关系，并进行补偿。

**通用公式示例:**

1.  **基于视场角 (FOV) 和像素位置的补偿 (简化模型):**
    假设摄像头具有水平视场角 `FOV_H`，图像宽度为 `W_pixels`。物体在图像中的水平像素坐标为 `px` (从图像中心算起，中心为0，左负右正)。
    物体距离摄像头中心的实际横向偏移 `X_world` 可以近似为：
    `X_world = D_corrected * tan( (px / (W_pixels / 2)) * (FOV_H / 2) )`
    其中:
    *   `D_corrected`: 经过非线性修正后的物体距离。
    *   `FOV_H`: 摄像头的水平视场角 (弧度)。
    *   `W_pixels`: 图像的水平总像素数。
    *   `px`: 物体中心在图像中的水平像素坐标，相对于图像中心 (例如, `px = current_pixel_x - W_pixels / 2`)。

    这个公式将像素偏移转换为角度偏移，然后利用已知的距离 `D_corrected` 来估算实际的横向偏移。

2.  **基于多项式拟合的横向位置修正:**
    类似于距离修正，可以对观察到的像素横坐标 `px` 和实际横向偏移 `X_world` 之间的关系进行多项式拟合：
    `X_world = p_2 * px^2 + p_1 * px + p_0`
    或者，如果已知 `X_world`，反过来修正图像上的目标中心点，以用于更精确的瞄准：
    `px_corrected = q_2 * X_world^2 + q_1 * X_world + q_0`
    系数 `p_i` 或 `q_i` 通过标定获得。

**实现要点:**
*   **镜头畸变模型:** 更精确的补偿需要考虑镜头的径向和切向畸变。OpenCV 提供了完整的摄像头标定和畸变校正功能。如果使用了这些功能对图像进行了预处理，则上述简化模型的适用性会更好。
*   **标定:** 同样需要精确标定。可以在不同已知横向位置和距离处放置目标，记录其图像坐标，然后拟合模型参数。
*   **与距离修正的耦合:** 横向畸变补偿的效果通常依赖于准确的距离估计，因此它应该在距离修正之后应用，或者两者联合优化。

**保证视野范围:**
这些修正算法的有效性高度依赖于在其预期工作的视野范围内的精确标定。如果机器人需要在非常广阔的视野或极端的近/远距离下工作，可能需要分段函数或更复杂的模型来保证精度。关键在于通过实验数据验证和优化模型参数。


### 3.4. 高效的性能优化与系统资源管理

为确保视觉处理模块在提供强大功能的同时，不会过度消耗宝贵的控制系统资源或导致主控制循环延迟，我们实施了多项性能优化措施：
*   **图像降采样 (`DOWNSCALE_FACTOR`):** 允许在图像处理的初始阶段按比例缩小图像尺寸。这是一种在处理精度和计算速度之间取得平衡的有效策略，显著减少了后续各处理步骤的计算负荷。
*   **关键几何信息预计算:** 对于如“目标抓取区域”这类在单次运行中相对固定的复杂几何参数，系统会在图像处理管线初始化时便完成计算和缓存，避免了在每一帧图像处理中重复计算所带来的性能开销。
*   **精细化的内存管理 (OpenCV `Mat` 对象):** 严格遵循OpenCV `Mat` 对象的使用规范，确保图像数据占用的内存在使用完毕后能够被及时、正确地分配和显式释放 (`releaseMats()`)，有效防止了内存泄漏问题，保证了系统的长期稳定运行。
*   **线程安全的数据同步 (`volatile` 关键字):** 在 `VisionGraspingAPI` 中，用于在视觉处理线程和主控线程间传递最新识别结果的 `latestResult` 变量被声明为 `volatile`。这确保了该变量在多线程环境下的可见性和原子性，有效避免了数据竞争和不一致的问题。

### 3.5. 灵活且贴近实际的“可抓取区域”定义

准确界定机器人实际能够有效操作的抓取范围至关重要。本系统中的“可抓取区域”并非一个简单的、固定的屏幕像素框，而是基于机器人实际机械臂工作范围和摄像头视野的一种更贴近物理现实的定义：
*   **基于真实世界单位的配置:** 在 `SamplePipeline.java` 中，`drawTargetZoneCm` 方法（及其相关的预计算逻辑）允许开发者使用实际的物理单位（如厘米）来定义这个区域的各项几何参数（例如，区域的宽度、近端和远端距离等）。
*   **动态投影到图像平面:** 系统随后会将这个以真实世界坐标定义的区域，根据当前的摄像头标定参数和姿态，动态地投影到二维的摄像头图像平面上，形成一个通常为矩形与半圆形组合的特定形状。
这种设计使得“可抓取区域”的定义不仅非常直观，而且能够轻松适应机器人结构或摄像头安装位置的调整，无需繁琐地重新计算像素坐标。其结果是，系统能够更准确地判断哪些被识别出的目标是机器人真正有能力抓取的，从而提高了决策的有效性。

## 4. 调试与校准支持

该系统的设计充分考虑了可测试性和校准的便捷性：

*   **`ENABLE_DEBUG_VIEW`:** `VisionConstants.java` 中的一个全局标志，启用后会在FTC Dashboard摄像头画面上绘制详细的覆盖信息。包括检测到的轮廓、边界框、计算出的目标区域和参考网格，在开发和调试过程中提供宝贵的视觉反馈。
*   **丰富的遥测数据 (Telemetry):** `SamplePipeline`、`VisionGraspingCalculator` 以及示例程序 `VisionGraspingTeleOp` 都会输出丰富的遥测数据，显示中间值、评分和最终结果。
*   **集中的常量配置:** 所有关键参数都位于 `VisionConstants.java` 中，使校准工作更集中。项目 `readme.md` 文件为关键值（如 `REAL_WORLD_VIEW_WIDTH_CM` 和HSV颜色范围）的校准提供了清晰的指导。

## 5. 总结

视觉抓取系统是一项重要的工程成果，为FTC环境中的物体识别和操作提供了功能强大、稳定可靠且智能化的解决方案。其模块化设计、创新的问题解决方法以及对驾驶员辅助的关注，为我们提供了竞争优势。该系统文档齐全，并为适应未来比赛挑战的持续发展和调整做好了准备。

## 6. 核心运动学API分析

项目代码库中包含两个核心的运动学API，用于计算和转换机器人机构的位置与姿态。

### 6.1. `PositionCalculator.java`

**目的:**
`PositionCalculator` 类提供了一个静态方法 `calculatePositionValue`，用于根据一系列输入参数（最小/最大位置、旋转角度、用户输入值、反向标志）计算一个标准化的位置值。输出结果被归一化到 `[0, 1)` 范围。

**设计思想:**
该API旨在提供一个通用的、可配置的转换逻辑，将用户输入（可能来自摇杆、传感器或其他控制源）映射到一个标准化的输出范围。这种标准化输出非常适合用于控制舵机（其位置通常用0到1表示）或其他需要比例输入的执行器。
*   `rotateAngle` 参数充当用户输入的缩放因子。
*   `positionMin` 和 `positionMax` 定义了原始输入值在映射前的逻辑范围或偏移。
*   `reverse` 标志允许轻松反转映射关系。
*   最终的模1操作确保输出始终在 `[0, 1)` 区间内循环，这对于周期性运动或防止超出范围的舵机指令很有用。

**核心公式:**

1.  **中间项计算 (Common Term):**
    `commonTerm = ((positionMax - positionMin) / rotateAngle) * userInput`
    *   `positionMin`: 位置的最小值。
    *   `positionMax`: 位置的最大值。
    *   `rotateAngle`: 旋转角度（作为分母，不应为零）。
    *   `userInput`: 用户输入值。

2.  **原始结果计算 (Raw Result):**
    *   **标准模式 (`reverse == false`):**
        `rawResult = commonTerm + positionMin`
    *   **反向模式 (`reverse == true`):**
        `rawResult = 1.0 - commonTerm - positionMin`

3.  **归一化 (Normalization):**
    `normalizedResult = rawResult % 1.0`
    `if (normalizedResult < 0) { normalizedResult += 1.0; }`
    这将确保最终结果落在 `[0, 1)` 区间。

**使用场景:**
可用于将游戏手柄的轴输入（例如 `-1` 到 `1`）或自定义逻辑计算出的值，转换为舵机的目标位置（`0` 到 `1`）。通过调整 `positionMin`, `positionMax`, 和 `rotateAngle`，可以精细控制输入的灵敏度和映射范围。

### 6.2. `ServoKinematics.java`

**目的:**
`ServoKinematics` 类用于解算一个特定的连杆机构（可能是一个曲柄滑块或类似的四杆机构），根据期望的线性伸出距离，计算出驱动该机构的曲柄舵机所需旋转的角度以及对应的舵机逻辑位置（通常在0.0到1.0之间）。

**设计思想:**
该API封装了特定连杆机构的逆运动学解算。它基于机构的几何尺寸常量，通过三角函数和几何关系，将用户期望的末端执行器线性位移（`targetExtensionCm`）转换为主驱动臂（曲柄）的角度。然后，这个角度变化被映射到舵机的标准控制范围。
*   **几何参数化:** 机构的关键尺寸（如曲柄长度 `R_CRANK_LENGTH_CM`、连杆长度 `L_CONNECTING_ROD_LENGTH_CM`、垂直偏移 `H_EFFECTIVE_VERTICAL_OFFSET_CM` 等）被定义为常量，便于调整和标定。
*   **运动范围限制:** 考虑了机构的最大物理行程 (`MAX_EXTENSION_CM`) 和舵机对应的逻辑位置范围 (`SERVO_POSITION_AT_MIN_EXTENSION`, `SERVO_POSITION_AT_MAX_EXTENSION`)。
*   **错误处理:** 对几何上不可达的目标伸出距离进行检查并返回 `null` 或错误信息。
*   **结果封装:** 计算结果 (`rotationDegrees`, `servoPosition`) 被封装在 `ServoTarget` 内部类中，方便使用。

**核心公式与计算流程 (在 `calculateServoTarget` 和 `calculateCrankAngleRad` 方法中):**

1.  **目标点C的绝对X坐标计算:**
    `xCTargetAbsCm = X_C_RETRACTED_RELATIVE_TO_A_CM + targetExtensionCm`
    *   `X_C_RETRACTED_RELATIVE_TO_A_CM`: 完全收回时C点相对于A点的X坐标。
    *   `targetExtensionCm`: 期望的伸出距离。

2.  **计算曲柄角度 `theta_A` (在 `calculateCrankAngleRad` 中，对应于 `xCTargetAbsCm`):**
    假设A为曲柄旋转中心，B为曲柄与连杆的连接点，C为连杆末端（滑块）。
    *   **计算AC连线长度 `dAc`:**
        `dAc = sqrt(xCTargetAbsCm^2 + H_EFFECTIVE_VERTICAL_OFFSET_CM^2)`
        (基于C点相对A点的X坐标 `xCTargetAbsCm` 和垂直偏移 `H_EFFECTIVE_VERTICAL_OFFSET_CM`)
    *   **计算AC连线与X轴的夹角 `alpha`:**
        `alpha = atan2(-H_EFFECTIVE_VERTICAL_OFFSET_CM, xCTargetAbsCm)`
        (注意 `H_EFFECTIVE_VERTICAL_OFFSET_CM` 在 `atan2` 的 `y` 参数中取负)
    *   **计算三角形ABC中，角BAC (即 `beta`)，使用余弦定理:**
        `cosBetaArgNumerator = R_CRANK_LENGTH_CM^2 + dAc^2 - L_CONNECTING_ROD_LENGTH_CM^2`
        `cosBetaArgDenominator = 2 * R_CRANK_LENGTH_CM * dAc`
        `cosBetaArg = cosBetaArgNumerator / cosBetaArgDenominator` (带边界检查 `max(-1, min(1, ...))`) 
        `beta = acos(cosBetaArg)`
    *   **最终曲柄角度 (弧度):**
        `crankAngleRad = alpha + beta`

3.  **计算舵机旋转量和逻辑位置 (在 `calculateServoTarget` 中):**
    *   `initialAngleRad`: 对应 `targetExtensionCm = 0` 时的曲柄角度。
    *   `maxExtensionAngleRad`: 对应 `targetExtensionCm = MAX_EXTENSION_CM` 时的曲柄角度。
    *   `targetAngleRad`: 对应当前 `targetExtensionCm` 时的曲柄角度。
    *   **所需旋转角度 (弧度):**
        `rotationRequiredRad = targetAngleRad - initialAngleRad`
    *   **所需旋转角度 (度数, 归一化到0-360):**
        `normalizedRotationDeg = (toDegrees(rotationRequiredRad) % 360 + 360) % 360`
    *   **总可用角度行程 (弧度):**
        `totalAngleRangeRad = maxExtensionAngleRad - initialAngleRad`
    *   **当前行程占总行程的比例:**
        `travelFraction = rotationRequiredRad / totalAngleRangeRad`
    *   **舵机逻辑位置:**
        `servoPositionRange = SERVO_POSITION_AT_MAX_EXTENSION - SERVO_POSITION_AT_MIN_EXTENSION`
        `servoPosition = SERVO_POSITION_AT_MIN_EXTENSION + (travelFraction * servoPositionRange)`

**使用场景:**
用于精确控制通过舵机驱动的连杆机构，实现特定的线性伸缩运动。例如，控制机器人的抓取臂、提升机构或任何需要将舵机旋转转换为精确线性位移的子系统。
